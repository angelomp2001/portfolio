{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRW4gMTrk0ri"
      },
      "outputs": [],
      "source": [
        "\n",
        "#libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "#import data\n",
        "df = pd.read_csv('users_behavior.csv')\n",
        "print(df.head())\n",
        "print(df.describe())\n",
        "\n",
        "# QC data quality\n",
        "def view(dfs, view=None):\n",
        "    # Convert input to a dictionary of DataFrames if needed\n",
        "    if isinstance(dfs, pd.DataFrame):\n",
        "        dfs = {'df': dfs}  # Wrap single DataFrame in a dict with a default name\n",
        "    elif isinstance(dfs, pd.Series):\n",
        "        series_name = dfs.name if dfs.name is not None else 'Series'\n",
        "        dfs = {series_name: dfs.to_frame()}\n",
        "    else:\n",
        "        print(\"Input must be a pandas DataFrame or Series.\")\n",
        "        return\n",
        "\n",
        "    views = {\n",
        "        \"headers\": [],\n",
        "        \"values\": [],\n",
        "        \"missing values\": [],\n",
        "        \"dtypes\": [],\n",
        "        \"summaries\": []\n",
        "    }\n",
        "\n",
        "    missing_cols = []\n",
        "\n",
        "    for df_name, df in dfs.items():\n",
        "        for col in df.columns:\n",
        "            # Ensure we don't fail on empty columns\n",
        "            counts = df[col].value_counts()\n",
        "            common_unique_values = counts.head(5).index.tolist() if not counts.empty else []\n",
        "            rare_unique_values = df[col].value_counts(sort=False).head(5).index.tolist() if not counts.empty else []\n",
        "            if df[col].count() > 0:\n",
        "                data_type = type(df[col].iloc[0])\n",
        "            else:\n",
        "                data_type = np.nan\n",
        "\n",
        "            series_count = df[col].count()\n",
        "            no_values = len(df) - series_count\n",
        "            total = no_values + series_count\n",
        "            no_values_percent = (no_values / total) * 100 if total != 0 else 0\n",
        "\n",
        "            views[\"headers\"].append({\n",
        "                'DataFrame': f'{df_name}',\n",
        "                'Column': col,\n",
        "                'Common Values': common_unique_values,\n",
        "            })\n",
        "\n",
        "            views[\"values\"].append({\n",
        "                'DataFrame': f'{df_name}',\n",
        "                'Column': col,\n",
        "                'Rare Values': rare_unique_values,\n",
        "            })\n",
        "\n",
        "            views[\"missing values\"].append({\n",
        "                'DataFrame': f'{df_name}',\n",
        "                'Column': col,\n",
        "                'Series Count': series_count,\n",
        "                'Missing Values (%)': f'{no_values} ({no_values_percent:.0f}%)'\n",
        "            })\n",
        "\n",
        "            views[\"dtypes\"].append({\n",
        "                'DataFrame': f'{df_name}',\n",
        "                'Column': col,\n",
        "                'Common Values': common_unique_values,\n",
        "                'Data Type': data_type,\n",
        "            })\n",
        "\n",
        "            views[\"summaries\"].append({\n",
        "                'DataFrame': f'{df_name}',\n",
        "                'Column': col,\n",
        "                'Common Values': common_unique_values,\n",
        "                'Rare Values': rare_unique_values,\n",
        "                'Data Type': data_type,\n",
        "                'Series Count': series_count,\n",
        "                'Missing Values': f'{no_values} ({no_values_percent:.0f}%)'\n",
        "            })\n",
        "\n",
        "            if no_values > 0:\n",
        "                missing_cols.append(col)\n",
        "\n",
        "    code = {\n",
        "        'headers': \"# df.rename(columns={col: col.lower() for col in df.columns}, inplace=True)\",\n",
        "        'values': \"# df['column_name'].replace(to_replace='old_value', value=None, inplace=True)\\n# df['col_1'] = df['col_1'].fillna('Unknown', inplace=False)\",\n",
        "        'missing values': f\"# Check for duplicates or summary statistics\\nMissing Columns: {missing_cols}\",\n",
        "        'dtypes': \"# df['col'] = df['col'].astype(str) (Int64), (float64) \\n# df['col'] = pd.to_datetime(df['col'], format='%Y-%m-%dT%H:%M:%SZ')\",\n",
        "        'summaries': f\"DataFrames: {list(dfs.keys())}\\ndf.duplicated().sum() \\ndf.drop_duplicates() \\ndf.duplicated().sum()\"\n",
        "    }\n",
        "\n",
        "    if view is None or view == \"all\":\n",
        "        for view_name, view_data in views.items():\n",
        "            print(f'{view_name}:\\n{pd.DataFrame(view_data)}\\n{code.get(view_name, \"\")}\\n')\n",
        "    elif view in views:\n",
        "        print(f'{view}:\\n{pd.DataFrame(views[view])}\\n{code.get(view, \"\")}\\n')\n",
        "    else:\n",
        "        print(\"Invalid view. Available views are: headers, values, dtypes, missing values, summaries, or all.\")\n",
        "\n",
        "view(df,'missing values')\n",
        "\n",
        "\n",
        "# model_picker parameter optimizer:\n",
        "def hyperparameter_optimizer(model, param_name, low, high, train_features, train_target, valid_features, valid_target, model_name, tolerance=0.1):\n",
        "    best_score = -np.inf\n",
        "    best_param = None\n",
        "    print(f'Model name:{model_name}\\n')\n",
        "\n",
        "    while high - low > tolerance:\n",
        "        mid = (low + high) / 2\n",
        "\n",
        "        # Set the parameter value\n",
        "        params = {param_name: int(round(mid))}\n",
        "        model.set_params(**params)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(train_features, train_target)\n",
        "\n",
        "        # Score the model\n",
        "        score = model.score(valid_features, valid_target)\n",
        "\n",
        "        # Print current param and score for debugging\n",
        "        print(f\"Param {param_name}: {int(round(mid))}, Score: {score}\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_param = int(round(mid))\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid\n",
        "\n",
        "    return best_param, best_score\n",
        "\n",
        "#model picker\n",
        "def model_picker(features, target):\n",
        "    # Split data\n",
        "    df = pd.concat([features, target], axis=1)  # Ensure features and target are combined into a single DataFrame\n",
        "    df_train, df_other = train_test_split(df, test_size=0.4, random_state=12345)  # training is 60%\n",
        "    df_valid, df_test = train_test_split(df_other, test_size=0.5, random_state=12345)  # valid & test = .4 * .5 = .2 each\n",
        "    print(f'\\\n",
        "    df_train:{df_train.shape}\\n\\\n",
        "    df_valid: {df_valid.shape}\\n\\\n",
        "    df_test:{df_test.shape}')\n",
        "\n",
        "    # Define features and targets per df\n",
        "    train_features = df_train.drop(target.name, axis=1)\n",
        "    train_target = df_train[target.name]\n",
        "    valid_features = df_valid.drop(target.name, axis=1)\n",
        "    valid_target = df_valid[target.name]\n",
        "    test_features = df_test.drop(target.name, axis=1)\n",
        "    test_target = df_test[target.name]\n",
        "\n",
        "    # Define base models\n",
        "    dtc_model = DecisionTreeClassifier(random_state=12345)\n",
        "    rfc_model = RandomForestClassifier(random_state=12345)\n",
        "    lr_model = LogisticRegression(random_state=12345, solver='liblinear', max_iter=200)\n",
        "\n",
        "    # Optimize max_depth for DecisionTreeClassifier\n",
        "    dtc_max_depth, dtc_best_score = hyperparameter_optimizer(\n",
        "        dtc_model, 'max_depth', 1, 20,\n",
        "        train_features=train_features, train_target=train_target,\n",
        "        valid_features=valid_features, valid_target=valid_target, model_name = \"DecisionTreeClassifier\"\n",
        "    )\n",
        "\n",
        "    # Optimize max_depth and n_estimators for RandomForestClassifier\n",
        "    rfc_max_depth, _ = hyperparameter_optimizer(\n",
        "        rfc_model, 'max_depth', 1, 20,\n",
        "        train_features=train_features, train_target=train_target,\n",
        "        valid_features=valid_features, valid_target=valid_target, model_name = \"RandomForestClassifier\"\n",
        "    )\n",
        "    rfc_n_estimators, rfc_best_score = hyperparameter_optimizer(\n",
        "        rfc_model, 'n_estimators', 10, 100,\n",
        "        train_features=train_features, train_target=train_target,\n",
        "        valid_features=valid_features, valid_target=valid_target, model_name = \"RandomForestClassifier\"\n",
        "    )\n",
        "    rfc_model.set_params(max_depth=rfc_max_depth, n_estimators=rfc_n_estimators)\n",
        "\n",
        "    # Fit Logistic Regression model (no hyperparameter optimization in this setup)\n",
        "    lr_model.fit(train_features, train_target)\n",
        "    lr_model_score = lr_model.score(valid_features, valid_target)\n",
        "\n",
        "    # Determine the best model based on validation scores\n",
        "    best_scores = {\n",
        "        'DecisionTreeClassifier': dtc_best_score,\n",
        "        'RandomForestClassifier': rfc_best_score,\n",
        "        'LogisticRegression': lr_model_score\n",
        "    }\n",
        "    best_model_name = max(best_scores, key=best_scores.get)\n",
        "\n",
        "    # Retrieve the best model\n",
        "    if best_model_name == 'DecisionTreeClassifier':\n",
        "        best_model = dtc_model\n",
        "        best_model.set_params(max_depth=dtc_max_depth)\n",
        "    elif best_model_name == 'RandomForestClassifier':\n",
        "        best_model = rfc_model\n",
        "    else:\n",
        "        best_model = lr_model\n",
        "\n",
        "    # Refit the best model on the full training data\n",
        "    best_model.fit(train_features, train_target)\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    best_model_test_score = best_model.score(test_features, test_target)\n",
        "\n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "    print(f\"Optimal Hyperparameters: {best_model.get_params()}\")\n",
        "    print(f\"Test Score: {best_model_test_score}\")\n",
        "\n",
        "    return best_model, best_model_test_score\n",
        "\n",
        "# define target and features\n",
        "target = df['is_ultra']\n",
        "features = df.drop(target.name,axis = 1)\n",
        "\n",
        "# results\n",
        "best_model, best_score = model_picker(features, target)\n",
        "\n",
        "#double check\n",
        "#split data\n",
        "df = pd.concat([features, target], axis=1)\n",
        "df_train, df_other = train_test_split(df, test_size=0.4, random_state=12345, stratify = df['is_ultra'])\n",
        "df_valid, df_test = train_test_split(df_other, test_size=0.5, random_state=12345, stratify = df_other['is_ultra'])\n",
        "\n",
        "# define train target and features\n",
        "train_target = df_train['is_ultra']\n",
        "train_features = df_train.drop(target.name,axis = 1)\n",
        "\n",
        "# define test target and features\n",
        "test_target = df_test['is_ultra']\n",
        "test_features = df_test.drop(test_target.name,axis = 1)\n",
        "\n",
        "#fit model using optimial hyperparameters found by hyperparameter optimizer\n",
        "Optimal_Hyperparameters = {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 12, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 78, 'n_jobs': None, 'oob_score': False, 'random_state': 12345, 'verbose': 0, 'warm_start': False}\n",
        "rfc_model = RandomForestClassifier(**Optimal_Hyperparameters)\n",
        "rfc_model.fit(train_features,train_target)\n",
        "\n",
        "#predict y_hat\n",
        "df_test['y_hat'] = rfc_model.predict(test_features)\n",
        "\n",
        "# measure accuracy\n",
        "df_test['error'] = np.where(df_test['y_hat'] != test_target,1,0)\n",
        "\n",
        "# array\n",
        "error = np.array(df_test['error'])\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy = (len(error) - np.sum(error))/len(error)\n",
        "print(f'accuracy:{accuracy}')\n",
        "\n",
        "#sanity check using average\n",
        "average = df_test['is_ultra'].mean()\n",
        "\n",
        "# model performance = how much better accuracy is than average, as a multiple.\n",
        "model_performance = accuracy/average\n",
        "print(f'average:{average}\\nmodel_performance:{model_performance}')\n",
        "\n",
        "# saninty check with DummyClassifier\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\", random_state=0)\n",
        "dummy_clf.fit(train_features, train_target)\n",
        "df_test['dummy_y_hat'] = dummy_clf.predict(test_features)\n",
        "baseline_accuracy = accuracy_score(test_target, df_test['dummy_y_hat'])\n",
        "print(\"Baseline Accuracy:\", baseline_accuracy)\n",
        "\n",
        "'''\n",
        "Conclusion:\n",
        "We wanted to develop a model that would predict 'is_ulta' with the highest possible accuracy, with a threshold for accuracy at 0.75. the target was 30% 1s, and 70% 0s. We needed to take this into account in a two ways:\n",
        "1. split data accounting for this distribution.\n",
        "2. test model quality accounting for this distribution.\n",
        "\n",
        "Splitting training, validation, and test data to account for this distribution was done using the 'stratify' parameter in train_test_split(). The purpose of this parameter is to retain the proportion of values in the target.\n",
        "\n",
        "Testing model quality took into account the distribution of the target by comparing the model predictions to a naive (no features applied) prediction. This 'dummy' prediction would be the equivalent of guessing the target by just simply keeping the proportions. As a result, it is the equivalent of having no model and makes for a suitable 'baseline' benchmark to measure our 'informed' model against.\n",
        "\n",
        "The conclusion is that our model outperformed the baseline model by 12% (81% vs 69%). If a 5% outperformance is the threshold, we exceed this threshold by over 2x, suggesting the model was worth building.\n",
        "'''"
      ]
    }
  ]
}