{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB__f1s4nrf-"
      },
      "outputs": [],
      "source": [
        "''' Pick the best model for predicting binary classifier with a significant minority ratio, under various compensation strategies.\n",
        "compensation strategies: 'balanced weights' logistic regression setting, upsampling, downsampling'''\n",
        "\n",
        "# libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from data_explorers import view, see\n",
        "from data_transformers import downsample, upsample, ordinal_encoder, missing_values, feature_scaler, categorical_encoder, data_splitter, data_transformer\n",
        "from best_model_picker import optimizer, best_model_picker\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sys\n",
        "import pdb\n",
        "import inspect\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv('Churn.csv')\n",
        "\n",
        "## EDA\n",
        "view(df)\n",
        "\n",
        "\" 'I'll keep the header names, encode categorical, ['Exit'] has minority of 20%, which I think is fine. especially out of 10k rows.   \"\n",
        "\n",
        "# columns=['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
        "df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "see(df)\n",
        "\n",
        "## data transformation:\n",
        "# encode categorical\n",
        "# NOTE: ['Exit'] has minority of 20% and will stay that way:\n",
        "\n",
        "#define target & identify ordinal categorical vars\n",
        "target = df['Exited']\n",
        "features = df.drop(target.name, axis = 1)\n",
        "\n",
        "random_state = 99999\n",
        "metric = None\n",
        "model_options = {\n",
        "            'Regressions': {\n",
        "                'LogisticRegression': LogisticRegression(random_state=random_state, solver='liblinear', max_iter=200)},\n",
        "            'Machine Learning': {\n",
        "                'DecisionTreeClassifier': DecisionTreeClassifier(random_state=random_state),\n",
        "                'RandomForestClassifier': RandomForestClassifier(random_state=random_state),\n",
        "\n",
        "            }\n",
        "        }\n",
        "\n",
        "#raw\n",
        "print(f'raw...')\n",
        "best_scores_summary_df, optimized_hyperparameters, best_scores_by_model, model_scores, transformed_data, model_options = best_model_picker(\n",
        "    features = features,\n",
        "    target = target,\n",
        "    n_target_majority = None,\n",
        "    n_target_minority = None,\n",
        "    n_rows = None,\n",
        "    ordinal_cols = None,\n",
        "    random_state = random_state,\n",
        "    model_options = model_options,\n",
        "    split_ratio = (0.6, 0.2, 0.2),\n",
        "    missing_values_method= 'drop',\n",
        "    fill_value = None,\n",
        "    target_threshold = 0.5,\n",
        "    metric=metric,\n",
        "    target_type='classification'\n",
        ")\n",
        "raw_validation_scores = model_scores\n",
        "\n",
        "# balanced logistic regression\n",
        "model_options = {\n",
        "    'Regressions': {\n",
        "        'LogisticRegression': LogisticRegression(random_state=12345, solver='liblinear', class_weight='balanced')\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f'class weight adjustment...')\n",
        "best_scores_summary_df, optimized_hyperparameters, best_scores_by_model, model_scores, transformed_data, model_options = best_model_picker(\n",
        "    features = features,\n",
        "    target = target,\n",
        "    n_rows = None,\n",
        "    n_target_majority = None,\n",
        "    ordinal_cols = None,\n",
        "    random_state = random_state,\n",
        "    model_options = model_options,\n",
        "    split_ratio = (0.6, 0.2, 0.2),\n",
        "    missing_values_method= 'drop',\n",
        "    fill_value = None,\n",
        "    target_threshold = 0.5,\n",
        "    metric=metric,\n",
        "    target_type='classification'\n",
        ")\n",
        "\n",
        "lr_balanced_validation_scores = model_scores\n",
        "\n",
        "# upsampling\n",
        "print(f'upsampling...')\n",
        "best_scores_summary_df, optimized_hyperparameters, best_scores_by_model, model_scores, transformed_data, model_options = best_model_picker(\n",
        "    features = features,\n",
        "    target = target,\n",
        "    n_rows = None,\n",
        "    n_target_majority = 5000,\n",
        "    ordinal_cols = None,\n",
        "    random_state = random_state,\n",
        "    model_options = None,\n",
        "    split_ratio = (0.6, 0.2, 0.2),\n",
        "    missing_values_method= 'drop',\n",
        "    fill_value = None,\n",
        "    target_threshold = 0.5,\n",
        "    metric=metric,\n",
        "    target_type='classification'\n",
        ")\n",
        "upsampling_scores = model_scores\n",
        "\n",
        "# downsampling\n",
        "\n",
        "print(f'downsampling...')\n",
        "best_scores_summary_df, optimized_hyperparameters, best_scores_by_model, model_scores, transformed_data, model_options = best_model_picker(\n",
        "    features = features,\n",
        "    target = target,\n",
        "    n_rows = 4000,\n",
        "    n_target_majority = .2*10000,\n",
        "    ordinal_cols = None,\n",
        "    random_state = random_state,\n",
        "    model_options = None,\n",
        "    split_ratio = (0.6, 0.2, 0.2),\n",
        "    missing_values_method= 'drop',\n",
        "    fill_value = None,\n",
        "    target_threshold = 0.5,\n",
        "    target_type='classification',\n",
        "    metric=metric\n",
        ")\n",
        "downsampling_scores = model_scores\n",
        "\n",
        "# applying all models but with optimal hyperparameters to test set:\n",
        "print(f'testing all models on test data...')\n",
        "best_scores_summary_df, optimized_hyperparameters, best_scores_by_model_df, model_scores, _ , model_options = best_model_picker(\n",
        "    features = transformed_data[2],\n",
        "    target = transformed_data[3],\n",
        "    test_features= transformed_data[4],\n",
        "    test_target= transformed_data[5],\n",
        "    random_state = random_state,\n",
        "    model_options= model_options,\n",
        "    model_params = optimized_hyperparameters,\n",
        "    target_threshold = 0.5,\n",
        "    missing_values_method= 'drop',\n",
        "    metric=metric,\n",
        "    target_type='classification',\n",
        ")\n",
        "\n",
        "# applying all models but with optimal hyperparameters to test set AND optimized target threshold:\n",
        "print(f'testing all models on test data...')\n",
        "best_scores_summary_df, optimized_hyperparameters, best_scores_by_model_df, model_scores, transformed_data, model_options = best_model_picker(\n",
        "    features = transformed_data[2],\n",
        "    target = transformed_data[3],\n",
        "    test_features= transformed_data[4],\n",
        "    test_target= transformed_data[5],\n",
        "    random_state = random_state,\n",
        "    model_options= model_options,\n",
        "    model_params = optimized_hyperparameters,\n",
        "    target_threshold = None,\n",
        "    missing_values_method= 'drop',\n",
        "    metric=metric,\n",
        "    target_type='classification',\n",
        ")\n",
        "\n",
        "'Conclusion: RandomForest is generally a superior model.  Downsampling was the best way to maximize Accuracy.'"
      ]
    }
  ]
}